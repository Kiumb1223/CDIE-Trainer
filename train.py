#!/usr/bin/env python
# -*- coding: utf-8 -*-
'''
@File     :     train.py
@Time     :     2025/11/21 12:04:07
@Author   :     Louis Swift
@Desc     :     
'''

import os 
os.environ["HYDRA_FULL_ERROR"] = "1"

import hydra
from loguru import logger
from core.detector import *
from omegaconf import OmegaConf
from core.loss import Criterion
from torch.optim import Adam,SGD
from utils.trainer import Trainer
from utils.logger import setup_logger
from utils.distributed import get_rank
from torch.utils.data import DataLoader
from core.model import EnhanceDetectNet
from torch.optim.lr_scheduler import MultiStepLR,ExponentialLR
from utils.misc import collect_env,get_exp_info,set_random_seed


@logger.catch
@hydra.main(config_path='configs', config_name='config.yaml',version_base=None)
def main(config:OmegaConf):

    # remove the 'train.log' generated by hydra
    os.remove('train.log')

    #---------------------------------#
    #  print some necessary infomation
    #---------------------------------#
    setup_logger(config.exp.work_dir,get_rank(),f'log_rank{get_rank()}.txt')
    logger.info("Environment info:\n" + collect_env())
    logger.info("Config info:\n" + get_exp_info(config))

    #---------------------------------#
    #  prepare training
    #---------------------------------#
    set_random_seed(config.exp.random_seed)
    train_dataset,test_dataset,collate_fn = build_dataset(config.detector.dataset)

    train_loader  = DataLoader(train_dataset,batch_size=config.exp.batch_size,shuffle=True,pin_memory=True,
                               num_workers=config.exp.num_workers,collate_fn=collate_fn,drop_last=True)

    valid_loader  = DataLoader(test_dataset,batch_size=config.exp.batch_size,shuffle=False,pin_memory=True,
                               num_workers=config.exp.num_workers,collate_fn=collate_fn,drop_last=True)
    
    model = EnhanceDetectNet(config.enhancer,config.detector).to(config.exp.device)
    model.train()

    optimizer = Adam(model.parameters(), lr=config.exp.lr,weight_decay=config.exp.weight_decay)
    # lr_scheduler = MultiStepLR(optimizer,milestones=config.MILLESTONES)
    # optimizer = SGD(model.parameters(), lr=config.LR,momentum=config.MOMENTUM,weight_decay=config.WEIGHT_DECAY)
    lr_scheduler = ExponentialLR(optimizer,gamma=config.exp.gamma)

    loss_func = Criterion(config.loss,config.enhancer,config.detector).to(config.exp.device)
    loss_func.float()
    trainer = Trainer(
        model=model,optimizer=optimizer,lr_scheduler=lr_scheduler,loss_func=loss_func,
        train_loader=train_loader,val_loader=valid_loader,
        **config.exp
    )
    #---------------------------------#
    #  start Training
    #---------------------------------#
    trainer.train()

if __name__ == '__main__':
    main()
    # cProfile.run('main()',filename='TimeAnalysis.out')